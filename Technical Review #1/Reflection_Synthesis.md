#***Reflection and Synthesis***

##**Feedback and decisions**

###1. ***Key Question #1: Cleaning Raw Data***
There are many missing values in our data set and we wanted to get different ideas on   how to deal with the missing values. One of the ideas was to make a training model on the data set with the other variables to predict the missing values. For example, if we have missing values in column “budget’, we can leverage other variables in the data set to predict the budget for the movies that do not have their budget listed. Others also suggested that we test our model with variables with and without any missing values, and try to find out if the missing values have any significant impact on the result. As we start building our model, we will try to see if any of these missing values actually have an impact on the result of our model. Our group believes that we shouldn’t make any prejudgement on the data set and missing values before we actually run the model. We will try to use various methods to go about missing values and then decide one which method is most appropriate.   

Our data set has movies from early 1900s and since we are trying to utilize number of Facebook likes for actors and directors as predictors for IMDb movie scores, our group wanted to filter only the movies that are released after 2006. One of the feedbacks we received is to run the model with all years included and only those released after 2006 to compare and see whether filtering the movies by years is significant. Other feedback we received is, rather than filter by years, use only observations that do not have missing values in number of Facebook likes. Based off the feedback, our group will try combinations of different filters on years and number of Facebook likes to explain majority of variability in the data set and get a good understanding of our sample.   

###2. ***Key Question #2: Variable 'Genre'***
Since each movie is associated with multiple genres and all the genres were written in one cell per movie, we had to decide whether we should randomly pick one of the genres, choose the first one listed, or use the dummy variables. During the technical review session, there were efficient feedbacks. One recommendation is to have the movies over representative in the data set, but they have to be represented the same number of times. For instance, if a movie has six genres in the data set, that movie should have 6 replicated rows with each distinctive genre. The other recommendation was to decide which genre is mostly associated with the theme, or context of the movie. Let’s take the movie Her into consideration. The genre of Her is listed as Drama/Sci-Fi. What Oliver told in the technical review is that since Her is more correlated to sci-fi genre, we should eliminate the drama and keep sci-fi. It is true that those recommendations were very helpful, however, we believed that those tasks would take too much time in filtering the genre. Therefore, we are going to create the dummy variable for each genre because it allows to sorting of data into mutually exclusive categories.

###3. ***Key Question #4: Other interests in the data set?***
Aside from the technical issues we wanted to get feedback on, we also wanted to gauge interest on what other types of insights our classmate wanted to know when thinking about IMDb scores (or movies in general) to help drive our actions with the data. Some of the classmates were interested in random connections or surprising elements of the data. For example, the scores for two movies are really similar, but many of the predictors show they’re very different types of movies. Another example is many movies with high IMDb scores could have a director’s name that begins with the letter ‘A’. To share interesting potential findings like the ones just mentioned, we would have to be very attentive to the data and assess relationships a lot. We would proceed by building histograms of relationships between predictors and the response as well as between predictors. Classmates are also interested in are how predictors that determine the IMDb scores have changed over the years. And even more in depth, if a movie from back then came out today, how would the IMDb score differ? To figure this out, we would assess the models for every ten years to see if the predictors determining the IMDb score changes and if so, use an example of a movie to show how the IMDb score would actually differ when using a model for a different set of years.  

##**Review process Reflection**
Overall, we pivoted our planned agenda and did not stick as closely to it as we anticipated. While we were able to give our project’s background and context, we spent more time on each key question than we had planned for. While this means that we were not able to get all our questions across, one of the positives that came out of this experience was that the suggestions we received are more detailed and in depth than they would have been had we stuck to our original planned discussion times. After presenting our first two key questions, we got a sense from the room that maybe all our questions were too stats heavy. Instead of presenting our third key question which was also statistics focused, we decided to present our fourth key question instead (completely skipping the third). This question was less stats focused and was well received by the class. A lot of people who had not been as vocal about the other questions shared their opinions in answering this question. Additionally, while we had planned to have students break up into groups to discuss each key question, this method ended up falling through. Part of this can be attributed to the fact that our group presented last, by then people seemed to be pretty worn out. However, our team was flexible in switching to a more laid back discussion with the entire class instead. For next time, our team will have a more dynamic set of questions that do not all revolve around the same topic and include key questions on software. This will help keep students engaged throughout the presentation. While our team overall felt that the feedback we received was really helpful as a result of us not following our planned discussion times for each key question too closely, moving forward in order to make sure we are able to address each key question that we had prepared, we will consider having a timer for each question.
